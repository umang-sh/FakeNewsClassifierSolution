{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Umang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.keras.layers import Embedding,Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/fake-news/train.csv\")\n",
    "test  = pd.read_csv(\"data/fake-news/test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data (20800, 5)\n",
      "Shape of test data (5200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data {}\".format(train.shape))\n",
    "print(\"Shape of test data {}\".format(test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the count of nulls in the data and filling them with blank spaces, this is done to make sure the later steps of Data pre-processing and model don't break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(\" \")\n",
    "test  = test.fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a synthetic feature called merged, by combining the title and author features together, this is done for 2 purposes:\n",
    "1. Classification of the news depends on both author and the content, hence a single feature would be better for learning \n",
    "2. The algorithm would only have a single feature to learn on.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"merged\"] = train[\"title\"]+\" \"+train[\"author\"]\n",
    "test[\"merged\"]  = test[\"title\"]+\" \"+test[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['label'],axis=1)\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copy of the train and test dataframes for data pre-processing\n",
    "messages = X.copy()\n",
    "messages.reset_index(inplace=True)\n",
    "messages_test = test.copy()\n",
    "messages_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing.We perform the following pre-processing steps:\n",
    "    1. All the punctuations/sequences are removed, they don't really help in the model learning\n",
    "    2. To prevent confusion and to ease the 1-hot encoding process, we convert everything to lower case\n",
    "    3. We then make a continous stream of tokens rather than sentences\n",
    "    4. Then we do stemming which is a common NLP pre-procesing step, it reduces the word to its root word for e.g chocolaty becomes chocolate\n",
    "    5. Lastly we create a 1-hot representaion of the text present, ML algorithms only understand numbers, hence we need to convert the text into numbers and this is one of the most simplest and efficient conversion.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flynn hillari clinton big woman campu breitbart daniel j flynn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def perform_preprocess(data):\n",
    "    '''Input: Data to be processed\n",
    "       Output: Preprocessed data\n",
    "    '''\n",
    "    corpus = []\n",
    "    for i in range(0,len(data)):\n",
    "        review = re.sub('[^a-zA-Z]',' ',data['merged'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus\n",
    "    \n",
    "train_corpus = perform_preprocess(messages)\n",
    "test_corpus  = perform_preprocess(messages_test)\n",
    "train_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to one-hot repr.\n",
    "vocab_size = 5000\n",
    "one_hot_train = [one_hot(word,vocab_size) for word in train_corpus]\n",
    "one_hot_test  = [one_hot(word,vocab_size) for word in test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding is an important step before we proceed to make models. padding makes the data uniform by making sure each feature vector is of same length by padding zeros. This becomes crucial because most algorithms(e.g neural networks) require the input data to be uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 20\n",
    "embedd_docs_train = pad_sequences(one_hot_train,padding='pre',maxlen=sent_length)\n",
    "embedd_docs_test  = pad_sequences(one_hot_test,padding='pre',maxlen=sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the final dataset to numpy arrays as most algorithm implementations need np arrays as inputs\n",
    "final_X = np.array(embedd_docs_train)\n",
    "final_y = np.array(y)\n",
    "x_test_final = np.array(embedd_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20800, 20), (20800,), (5200, 20))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_X.shape,final_y.shape,x_test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, test, validation split with 80.10,10 proportion for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(final_X, final_y, test_size=0.1, random_state=42, stratify = final_y)\n",
    "X_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n",
    "x_test_final = x_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first try a boosted Random forest classifier called XGBoost, this ML algorithm uses the power of boosting and delivers great results with very high speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Umang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:19:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1039\n",
      "           1       0.98      0.99      0.98      1041\n",
      "\n",
      "    accuracy                           0.98      2080\n",
      "   macro avg       0.98      0.98      0.98      2080\n",
      "weighted avg       0.98      0.98      0.98      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb= XGBClassifier()\n",
    "xgb.fit(X_train,Y_train)\n",
    "pred_xgb = xgb.predict(x_test)\n",
    "XGBmetrics    = classification_report(y_test,pred_xgb_xgb)\n",
    "print(XGBmetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly we try a Deep learning model as well, the thought behind is that in such text classification tasks, LSTMs have worked very well in the past, also we use an Embedding layer as the starting layer of our neural net. This creates a word embedding of the text input and has proven to work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 40)            200000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               56400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 256,501\n",
      "Trainable params: 256,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_feature_vector = 40\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_feature_vector,input_length=sent_length))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 14s 51ms/step - loss: 0.1947 - accuracy: 0.9162 - val_loss: 0.0402 - val_accuracy: 0.9882\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 13s 48ms/step - loss: 0.0258 - accuracy: 0.9921 - val_loss: 0.0273 - val_accuracy: 0.9931\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 13s 51ms/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.0283 - val_accuracy: 0.9941\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 13s 48ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0315 - val_accuracy: 0.9893\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 14s 52ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0350 - val_accuracy: 0.9882\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 13s 50ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0538 - val_accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 14s 52ms/step - loss: 4.0935e-04 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9909\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 14s 52ms/step - loss: 8.7034e-04 - accuracy: 0.9998 - val_loss: 0.0392 - val_accuracy: 0.9888\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 13s 50ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0393 - val_accuracy: 0.9882\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 13s 50ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0434 - val_accuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1409a78d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,validation_data=(x_valid,y_valid),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-cf006eeafd1c>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1039\n",
      "           1       0.99      0.98      0.99      1041\n",
      "\n",
      "    accuracy                           0.99      2080\n",
      "   macro avg       0.99      0.99      0.99      2080\n",
      "weighted avg       0.99      0.99      0.99      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_test)\n",
    "DLmetrics = classification_report(y_test,predictions)\n",
    "print(DLmetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the metrics of both ML and DL approches used above, one can see that LSTM based Deep learning model outperforms the XGBoost classifier. Hence we move ahead and use LSTM model for submission purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = pd.DataFrame(model.predict_classes(x_test_final))\n",
    "test_id = pd.DataFrame(test[\"id\"])\n",
    "submission = pd.concat([test_id,predictions_test],axis=1)\n",
    "submission.columns = [\"id\",\"label\"]\n",
    "submission.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
